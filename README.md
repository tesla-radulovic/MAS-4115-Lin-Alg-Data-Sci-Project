# MAS-4115-Lin-Alg-Data-Sci-Project
Investigation of Optimization Algorithms And Their Applications in Data Science Accelerated Random Search And Its Use in Noisy Linear Regression

Accelerated Random Search (ARS) is a slight variation on Pure Random Search (PRS, from here on out) which is several thousands of times quicker. Though there exist plenty of other random walk algorithms which converge not just faster than ARS, the main power of ARS does not come from its speed relative to other algorithms, rather its ease of implementation and broader general use cases (it works for all (continuous) cases for which PRS does). What was done in this project was research into a further (possible) improvement of ARS using techniques from Linear Algebra, as well as applying ARS to Linear Regression with different norms, as well as testing its viability in applications in Neural Networks.
    
ARS is designed specifically in mind to benefit in regions with large areas of relatively constant gradient. In such cases algorithms such as gradient descent waste time by traveling fixed steps with little improvement. ARS alleviates this by restoring the search domain to the entire space when an improvement is found, and for each subsequent non-improvement guess the search radius shrinks in half, centered at the last improvement. Most of the guesses in this stage are useless, however, the domain quickly shrinks to be just that of the region of relatively constant gradient. The chance of finding an improvement in the quasi-plane is roughly  ½. If an improvement is found it most likely jumps over a much larger distance than a gradient descent would. 
 
![apologies for the cropping, it looks much better on the google doc ;p](https://github.com/tesla-radulovic/volk-boilerplate/blob/main/MAS%204115%20Project.jpg?raw=true)
    
Pictured: Ideal case for ARS. Increasing the search radius (where we pick the next random sample) around a point in a constantly sloped region, does not decrease the chance of finding an improvement, and increases the rate of convergence. (Ignore the behavior of the function at the end)
ARS was implemented using numpy, for functions which take numpy arrays of arbitrary shape and rectangular domain. ARS was compared to PRS in finding the minimum of various functions such as quadratic functions and finding the inverses of matrices. Both problems have known constant time solutions; these were just test cases. Next, a potential improvement was developed: Quadratic ARS, or QARS. The idea being that when ARS is very close to the local minimum needless time is spent on far out random samples, instead the last few best samples are used to estimate a quadratic function and the next sample point is picked to be the vertex of the paraboloid. The implementation ended up being quite technical and time-consuming. QARS ended up not being the hail mary that was promised, it did not seem to work at all for some functions. But regardless, optimization is generally broad, how can this be applied in Data Science? ARS is mainly a tool that can be very valuable in doing experiments in Data Science. One possible use case, which was implemented was with a regression with arbitrary norm. Theory tells us that norms using absolute values, or even square roots are less sensitive to massive outliers in the data. So, using ARS to fit a series of noisy points roughly following a line, as well as a handful of completely unrelated outlier points in the data should perform better with L1 (absolute value) or L½ (roots of absolute value) than L2. And sure enough, this was the case. ARS was able to find parameters better fitting the generating parameters, with L1 and L½ norms than with L2.

Originally, the intent of the project was to apply ARS to neural networks, specifically to see if a similar improvement on fitting with significant outliers could be observed. The intuition was that back propagation was reliant on gradients and were not well-defined with L1 and L½, but it was discovered during research that this intuition must be wrong as Keras supported mean absolute error, as well as arbitrary user-defined loss functions. Regardless, ARS was tested on a neural network on the sklearn iris dataset with over 600 parameters, while it converged, it was extremely slow as the halving from the full domain upon every improvement took far too long in 600 dimensions. It took an hour for ARS to reach a loss that was 100 times greater than that found with prebuilt keras methods in seconds (this was omitted from the notebook as it would take far longer than on my computer). However, ARS was used to train a keras network of 65 parameters emulating an XOR gate, and ARS significantly outperformed traditional “adam” optimization when the parameters were adjusted so that both methods took roughly equal time. This definitely warrants further investigation on how ARS can be improved in the high-dimensional case, possibly by not scoping out to the entire domain, shrinking faster or accounting for direction.

